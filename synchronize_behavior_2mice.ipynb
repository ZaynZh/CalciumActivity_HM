{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import json\n",
    "import scipy\n",
    "from bisect import bisect_left\n",
    "import pickle as pkl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bpod_data(dpath):\n",
    "    with open(dpath, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def get_event_time(event, bpod_data):\n",
    "    \"\"\"\n",
    "    Input a event name and bpod_data. Return the time stamp of the event(relative to trial start)\n",
    "    \"\"\"\n",
    "    onsets = []\n",
    "    for trial in bpod_data[\"SessionData\"]['RawEvents']['Trial']:\n",
    "        if not trial['States']['WrongPort'][0]:\n",
    "            if type(trial['States'][event][0]) != float:\n",
    "                onsets.append(np.squeeze(trial['States'][event][0])[0])\n",
    "            else:\n",
    "                onsets.append(np.squeeze(trial['States'][event])[0])\n",
    "    return np.array(onsets)\n",
    "\n",
    "def take_closest(ms_stamps, bpod_stamps):\n",
    "    \"\"\"\n",
    "    Assumes ms_stamps is sorted. Returns closest stamps to the given bpod_stamps.\n",
    "\n",
    "    If two numbers are equally close, return the smallest number.\n",
    "    \"\"\"\n",
    "    msAlignedIndex = []\n",
    "    for bpod_stamp in bpod_stamps:\n",
    "        pos = bisect_left(ms_stamps, bpod_stamp)\n",
    "        if pos == 0:\n",
    "            return ms_stamps[0]\n",
    "        if pos == len(ms_stamps):\n",
    "            return ms_stamps[-1]\n",
    "        before = ms_stamps[pos - 1]\n",
    "        after = ms_stamps[pos]\n",
    "        if after - bpod_stamp < bpod_stamp - before:\n",
    "            msAlignedIndex.append(np.where(ms_stamps ==after))\n",
    "        else:\n",
    "            msAlignedIndex.append(np.where(ms_stamps ==before))\n",
    "    return np.squeeze(np.array(msAlignedIndex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse = 'ZZ0024-LR' \n",
    "mouse_pair = 'ZZ0024-L_ZZ0024-LR'\n",
    "date = '20240926'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_stamps = np.squeeze(scipy.io.loadmat(f\"/Users/fgs/HMLworkplace/Arena_analysis/Data/ws_data/processed/{mouse}/{mouse}_{date}_ms.mat\")['ms_frames_samplingstamps'])\n",
    "bpod_trialstart_stamps = np.squeeze(scipy.io.loadmat(f\"/Users/fgs/HMLworkplace/Arena_analysis/Data/ws_data/processed/{mouse}/{mouse}_{date}_bpod.mat\")['bpod_trialstart_samplingstamps'])\n",
    "\n",
    "bpod_data = load_bpod_data(f'/Users/fgs/HMLworkplace/Arena_analysis/Data/bpod_data/{mouse_pair}/{mouse_pair}_{date}.json')\n",
    "\n",
    "minian_results = xr.open_dataset(f\"/Users/fgs/HMLworkplace/Arena_analysis/Data/minian_data/{mouse}_{date}.netcdf\")\n",
    "calcium_traces = minian_results.C\n",
    "\n",
    "with open(f'/Users/fgs/HMLworkplace/Arena_analysis/Data/bahavior_led_frames/{mouse_pair}_{date}.pickle', 'rb') as f:\n",
    "    dlc_trialStartStamps = np.array(pkl.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3377, 3676, 3998, 4236, 4549, 4856, 5133, 5445, 5802, 6133, 6362])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "led_frames_sample = dlc_trialStartStamps[10:21]\n",
    "print(led_frames_sample[-1] - led_frames_sample[0])\n",
    "led_frames_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3832, 4129, 4450, 4688, 4999, 5304, 5580, 5889, 6246, 6576, 6804])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms_trialstarts = take_closest(ms_stamps,bpod_trialstart_stamps)\n",
    "ms_sample_trialstarts = ms_trialstarts[10:21]\n",
    "print(ms_sample_trialstarts[-1] - ms_sample_trialstarts[0])\n",
    "ms_sample_trialstarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_neurons = [22,21,32,114,125,156,171,172]\n",
    "calciam_traces = minian_results.C\n",
    "sample_C = calciam_traces[selected_neurons]\n",
    "sample_C = np.array(sample_C)[:,ms_sample_trialstarts[0]:ms_sample_trialstarts[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "2048\n",
      "Video saved at /Users/fgs/HMLworkplace/Arena_analysis/Data/behavior_videos/test.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Input video path (animal behavior video)\n",
    "behavior_video_path = f\"/Users/fgs/HMLworkplace/Arena_analysis/Data/behavior_videos/{mouse_pair}_{date}_highsat.mp4\"\n",
    "\n",
    "# Multiple neural signals (replace with your actual data)\n",
    "fps = 30  # Frames per second (match the behavior video)\n",
    "time = np.array(range(len(sample_C.T)))  # 10 seconds of data (for example)\n",
    "neurons = sample_C  # Example signals\n",
    "\n",
    "# Output video path\n",
    "output_video_path = \"/Users/fgs/HMLworkplace/Arena_analysis/Data/behavior_videos/test.mp4\"\n",
    "\n",
    "# Open behavior video\n",
    "behavior_cap = cv2.VideoCapture(behavior_video_path)\n",
    "\n",
    "# Get video properties\n",
    "behavior_width = int(behavior_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "behavior_height = int(behavior_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "total_frames = int(behavior_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(behavior_height)\n",
    "print(behavior_width)\n",
    "# Define the time window of the behavior video you want to use (in seconds)\n",
    "start_frame = led_frames_sample[0]\n",
    "end_frame = start_frame + len(sample_C.T)\n",
    "# Set the frame position to the start time\n",
    "behavior_cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "# Determine video dimensions for output\n",
    "num_neurons = len(neurons)\n",
    "neural_signal_width = 1024  # Width allocated for neural signals\n",
    "neural_signal_height = behavior_height // num_neurons  # Height allocated per neuron\n",
    "output_width = behavior_width + neural_signal_width\n",
    "output_height = behavior_height\n",
    "\n",
    "# Prepare output video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (output_width, output_height))\n",
    "\n",
    "# Fixed y-axis limits for neural signals\n",
    "#y_min, y_max = np.min(sample_C), np.max(sample_C) # Adjust based on your data range\n",
    "\n",
    "# Iterate through behavior video frames within the selected time range\n",
    "frame_index = start_frame\n",
    "ms_index = 0\n",
    "while frame_index < end_frame:\n",
    "    ret, frame = behavior_cap.read()\n",
    "    behavior_frame = cv2.resize(frame, (2048, 2048))\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Prepare a blank canvas for neural signals\n",
    "    neural_canvas = np.zeros((behavior_height, neural_signal_width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Plot each neuron's signal in its allocated row\n",
    "    for neuron_idx, signal in enumerate(neurons):\n",
    "        plt.figure(figsize=(4, 2), facecolor='black')  # Smaller figure for each neuron\n",
    "        start_idx = max(0, ms_index - 200)\n",
    "        end_idx = ms_index\n",
    "        plt.plot(time[start_idx:end_idx], signal[start_idx:end_idx], 'orange', lw=2)\n",
    "        plt.scatter(time[ms_index], signal[ms_index], color='orange', s=50)\n",
    "        y_min, y_max = np.min(sample_C[neuron_idx, :]), np.max(sample_C[neuron_idx, :]) \n",
    "        plt.ylim(y_min, y_max)\n",
    "        plt.title(f'Neuron{selected_neurons[neuron_idx]}', loc='left', fontsize=10, color='white')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Save the plot as an image\n",
    "        plot_path = f\"neuron_{neuron_idx}_frame_{ms_index}.png\"\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.savefig(plot_path, dpi=100, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "        # Load the image and resize it to fit the allocated row\n",
    "        signal_img = cv2.imread(plot_path)\n",
    "        signal_img = cv2.resize(signal_img, (neural_signal_width, neural_signal_height))\n",
    "        \n",
    "        # Place the signal image on the neural canvas\n",
    "        start_y = neuron_idx * neural_signal_height\n",
    "        end_y = start_y + neural_signal_height\n",
    "        neural_canvas[start_y:end_y, :] = signal_img\n",
    "        os.remove(plot_path)\n",
    "\n",
    "    # Combine the behavior video frame and neural signals\n",
    "    combined_frame = np.zeros((output_height, output_width, 3), dtype=np.uint8)\n",
    "    combined_frame[:, :2048] = behavior_frame  # Place behavior video\n",
    "    combined_frame[:, 2048:] = neural_canvas  # Place neural signals\n",
    "\n",
    "    # Write the combined frame to the output video\n",
    "    out.write(combined_frame)\n",
    "\n",
    "    frame_index += 1\n",
    "    ms_index += 1\n",
    "\n",
    "# Release resources\n",
    "behavior_cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"Video saved at {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1, 3.331]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpod_data['SessionData']['RawEvents']['Trial'][0]['States']['BackToCenter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
